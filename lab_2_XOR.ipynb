{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibnn/verydeep/blob/main/lab_2_XOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM52HYo8pU7r"
      },
      "outputs": [],
      "source": [
        "#XOR Gate using Multi Layer Perceptron and Tensor Flow\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_features = 2\n",
        "num_iter = 10000\n",
        "display_step = int(num_iter / 10)\n",
        "learning_rate = 0.01\n",
        "\n",
        "num_input = 2          # units in the input layer 28x28 images\n",
        "num_hidden1 = 2        # units in the first hidden layer\n",
        "num_output = 1         # units in the output, only one output 0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8_XNf8LpU7t"
      },
      "outputs": [],
      "source": [
        "#%% mlp function\n",
        "\n",
        "def multi_layer_perceptron_xor(x, weights, biases):\n",
        "\n",
        "    hidden_layer1 = tf.add(tf.matmul(x, weights['w_h1']), biases['b_h1'])\n",
        "    hidden_layer1 = tf.nn.sigmoid(hidden_layer1)\n",
        "\n",
        "    out_layer = tf.add(tf.matmul(hidden_layer1, weights['w_out']), biases['b_out'])\n",
        "\n",
        "    return out_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kBY1cDwpU7u",
        "outputId": "e47662e2-b4b9-4377-83a6-ef58cf5b6323"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "tf.placeholder() is not compatible with eager execution.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/faizan/uni/DL/Labs/LAB 2 Perceptron (MLP) and Sigmoid Neuron and XOR/LAB-2b.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/faizan/uni/DL/Labs/LAB%202%20Perceptron%20%28MLP%29%20and%20Sigmoid%20Neuron%20and%20XOR/LAB-2b.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(y, [\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m])                                    \u001b[39m# convert to 4x1\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/faizan/uni/DL/Labs/LAB%202%20Perceptron%20%28MLP%29%20and%20Sigmoid%20Neuron%20and%20XOR/LAB-2b.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# trainum_inputg data and labels\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/faizan/uni/DL/Labs/LAB%202%20Perceptron%20%28MLP%29%20and%20Sigmoid%20Neuron%20and%20XOR/LAB-2b.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mplaceholder(\u001b[39m'\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m'\u001b[39;49m, [\u001b[39mNone\u001b[39;49;00m, num_input])     \u001b[39m# training data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/faizan/uni/DL/Labs/LAB%202%20Perceptron%20%28MLP%29%20and%20Sigmoid%20Neuron%20and%20XOR/LAB-2b.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m Y \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mplaceholder(\u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39mNone\u001b[39;00m, num_output])\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py:3343\u001b[0m, in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3296\u001b[0m \u001b[39m\"\"\"Inserts a placeholder for a tensor that will be always fed.\u001b[39;00m\n\u001b[1;32m   3297\u001b[0m \n\u001b[1;32m   3298\u001b[0m \u001b[39m**Important**: This tensor will produce an error if evaluated. Its value must\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3340\u001b[0m \u001b[39m@end_compatibility\u001b[39;00m\n\u001b[1;32m   3341\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3342\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m-> 3343\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtf.placeholder() is not compatible with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3344\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39meager execution.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3346\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39mplaceholder(dtype\u001b[39m=\u001b[39mdtype, shape\u001b[39m=\u001b[39mshape, name\u001b[39m=\u001b[39mname)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
          ]
        }
      ],
      "source": [
        "\n",
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], np.float32)  # 4x2, input\n",
        "y = np.array([0, 1, 1, 0], np.float32)                      # 4, correct output, AND operation\n",
        "y = np.reshape(y, [4,1])                                    # convert to 4x1\n",
        "\n",
        "# trainum_inputg data and labels\n",
        "X = tf.placeholder('float', [None, num_input])     # training data\n",
        "Y = tf.placeholder('float', [None, num_output])    # labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrvA2H1qpU7v"
      },
      "outputs": [],
      "source": [
        "# weights and biases\n",
        "weights = {\n",
        "    'w_h1' : tf.Variable(tf.random_normal([num_input, num_hidden1])), # w1, from input layer to hidden layer 1\n",
        "    'w_out': tf.Variable(tf.random_normal([num_hidden1, num_output])) # w2, from hidden layer 1 to output layer\n",
        "}\n",
        "biases = {\n",
        "    'b_h1' : tf.Variable(tf.zeros([num_hidden1])),\n",
        "    'b_out': tf.Variable(tf.zeros([num_output]))\n",
        "}\n",
        "\n",
        "model = multi_layer_perceptron_xor(X, weights, biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFIPU0-OpU7w"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "- cost function and optimization\n",
        "- sigmoid cross entropy -- single output\n",
        "- softmax cross entropy -- multiple output, normalized\n",
        "'''\n",
        "loss_func = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=model, labels=Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_func)\n",
        "#train = tf.train.AdamOptimizer(0.1).minimize(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED74YZh5pU7y"
      },
      "outputs": [],
      "source": [
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "for k in range(num_iter):\n",
        "    tmp_cost, _ = sess.run([loss_func, optimizer], feed_dict={X: x, Y: y})\n",
        "    if k % display_step == 0:\n",
        "        #print('output: ', sess.run(model, feed_dict={X:x}))\n",
        "        print('loss= ' + \"{:.5f}\".format(tmp_cost))\n",
        "\n",
        "# separates the input space\n",
        "W = np.squeeze(sess.run(weights['w_h1']))   # 2x2\n",
        "b = np.squeeze(sess.run(biases['b_h1']))    # 2,\n",
        "\n",
        "sess.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydU8EfmEpU7z"
      },
      "outputs": [],
      "source": [
        "# Now plot the fitted line. We need only two points to plot the line\n",
        "plot_x = np.array([np.min(x[:, 0] - 0.2), np.max(x[:, 1]+0.2)])\n",
        "plot_y =  -1 / W[1, 0] * (W[0, 0] * plot_x + b[0])\n",
        "plot_y = np.reshape(plot_y, [2, -1])\n",
        "plot_y = np.squeeze(plot_y)\n",
        "\n",
        "plot_y2 = -1 / W[1, 1] * (W[0, 1] * plot_x + b[1])\n",
        "plot_y2 = np.reshape(plot_y2, [2, -1])\n",
        "plot_y2 = np.squeeze(plot_y2)\n",
        "\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y, s=100, cmap='viridis')\n",
        "plt.plot(plot_x, plot_y, color='k', linewidth=2)    # line 1\n",
        "plt.plot(plot_x, plot_y2, color='k', linewidth=2)   # line 2\n",
        "plt.xlim([-0.2, 1.2]); plt.ylim([-0.2, 1.25]);\n",
        "#plt.text(0.425, 1.05, 'XOR', fontsize=14)\n",
        "plt.xticks([0.0, 0.5, 1.0]); plt.yticks([0.0, 0.5, 1.0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8yBzb7ApU70"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}